# lightning.pytorch==2.1.1
seed_everything: 0
trainer:
  accelerator: auto
  strategy:
    class_path: lightning.pytorch.strategies.DDPStrategy
    init_args:
      find_unused_parameters: true

  devices: auto
  num_nodes: 1
  precision: 16-mixed
  logger:
    class_path: TensorBoardLogger
    init_args:
      save_dir: /home/xshadow/terratorch/logs/od_windturbine_dofav2_hpo/
      name: od_windturbine_dofav2_lr_0.0003_wd_0.005_warmup_2_e40
  callbacks:
    - class_path: LearningRateMonitor
      init_args:
        logging_interval: epoch
    - class_path: ModelCheckpoint
      init_args:
          mode: max
          monitor: val_map
          filename: best-{epoch:02d}
  max_epochs: 40
  check_val_every_n_epoch: 1
  log_every_n_steps: 50
  enable_checkpointing: true
  default_root_dir: /home/xshadow/terratorch/logs/
data:
  class_path: terratorch.datamodules.mWindTurbineDataModule
  init_args:
    batch_size: 8
    num_workers: 4
    img_size: 512
    root: /home/xshadow/terratorch/data/wind_turbine

model:
  class_path: terratorch.tasks.ObjectDetectionTask
  init_args:
    model_factory: ObjectDetectionModelFactory
    model_args:
      framework: faster-rcnn
      backbone: dofav2_large_patch14_224
      backbone_pretrained: true
      backbone_ckpt_data: https://hf.co/earthflow/DOFA/resolve/main/dofav2_vit_large_e150.pth
      backbone_pos_interpolation_mode: bicubic
      backbone_convert_patch_14_to_16: True
      num_classes: 2
      backbone_img_size: 512
      framework_min_size: 512
      framework_max_size: 512
      backbone_model_bands:
        - RED
        - GREEN
        - BLUE
      backbone_out_indices:
        - 5
        - 11
        - 17
        - 21
      necks:
        - name: ReshapeTokensToImage
        - name: LearnedInterpolateToPyramidal
        - name: FeaturePyramidNetworkNeck
        
    freeze_backbone: false
    freeze_decoder: false
    class_names:
      - Background
      - WindTurbine

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.0003
    weight_decay: 0.005

lr_scheduler:
  class_path: torch.optim.lr_scheduler.OneCycleLR
  init_args:
    max_lr: 0.0003
    total_steps: 40
    pct_start: 0.05
    anneal_strategy: "cos"
    div_factor: 10.0
    final_div_factor: 1000.0
